# -*- coding: utf-8 -*-
"""ENROLMENT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hjk1If5Hk4Eb2yLsH5LxInDpDGIf9U2u
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# file_path = "/content/drive/MyDrive/UNIVERSITY ENROLMENT/ENROLMENT.csv"
file_path = "E:\\AI,ML PROJECTS\\University Capacity enrollment Prediction\\ENROLMENT.csv"

data = pd.read_csv(file_path)
data.head()

"""# **Step 2: Data Exploration and Cleaning**

2.1. Understand the Data

2.2: Handle Missing Values


"""

# Understand the Data
# Display basic information about the DataFrame
print(data.info())

# Display summary statistics of the DataFrame
print(data.describe())

#2.2: Handle Missing Values
# Check for missing values
print(data.isnull().sum())

#Step 2.3: Data Cleaning
# Convert numerical columns to appropriate data types (if needed)
data['SELECTED'] = pd.to_numeric(data['SELECTED'], errors='coerce')
data['CAPACITY'] = pd.to_numeric(data['CAPACITY'], errors='coerce')
data['REGISTERED'] = pd.to_numeric(data['REGISTERED'], errors='coerce')

# Check for any anomalies in the data (e.g., negative values, values exceeding capacity, etc.)
print((data['SELECTED'] < 0).sum())  # Check for negative values in SELECTED
print((data['REGISTERED'] < 0).sum())  # Check for negative values in REGISTERED
print((data['REGISTERED'] > data['CAPACITY']).sum())  # Check for REGISTERED exceeding CAPACITY

# # Handle any anomalies as needed (e.g., removing or correcting rows)
# # Example: Removing rows where REGISTERED exceeds CAPACITY
# data = data[data['REGISTERED'] <= data['CAPACITY']]

# Display the cleaned DataFrame
print(data.head())

"""# **Step 3: Feature Engineering**

Feature engineering involves creating new features or transforming existing ones to improve the performance of machine learning models. Here are some ideas for feature engineering based on your dataset:

**Year Extraction:** Extract the starting year from the YEAR column.

**Course Popularity:** Calculate the popularity of each course based on the number of registered students.

**Capacity Utilization:** Calculate the utilization rate of the course capacity.

**Historical Trends:** Create lag features that capture historical trends in the data.

**3.1: YEAR EXTRACTION**
"""

#We can extract the starting year from the YEAR column for easier analysis.
# Extract the starting year from the 'YEAR' column
data['START_YEAR'] = data['YEAR'].apply(lambda x: int(x.split('/')[0]))

# Display the updated DataFrame
data.head()

"""**3.2 COURSE POPULARITY**"""

# Calculate the popularity of each course as the mean number of registered students
course_popularity = data.groupby('COURSE')['REGISTERED'].mean().reset_index()
course_popularity.columns = ['COURSE', 'COURSE_POPULARITY']

# Merge the popularity feature back into the original DataFrame
data = pd.merge(data, course_popularity, on='COURSE', how='left')

# Display the updated DataFrame
data.head()

"""**3.3 CAPACITY UTILIZATION**"""

# Calculate the capacity utilization rate
data['CAPACITY_UTILIZATION'] = data['REGISTERED'] / data['CAPACITY']

# Display the updated DataFrame
data.head()

"""**3.4 HISTORICAL TRENDS (Lag Features)**"""

#Create lag features to capture historical trends.
#For example, the number of registered students in the previous year.

# Sort data by course and start year
data = data.sort_values(by=['COURSE', 'START_YEAR'])

# Create lag features for the number of registered students
data['REGISTERED_LAG_1'] = data.groupby('COURSE')['REGISTERED'].shift(1)


# Fill any NaN values resulting from the lag operation
data = data.fillna(0)

# Display the updated DataFrame
data.head()

"""Explanation of above steps performed

Year Extraction: This step simplifies the YEAR column to a numeric starting year which can be used in model training.

Course Popularity: By calculating the average number of registered students per course, we create a feature indicating the popularity of a course.

Capacity Utilization: This feature represents how well the course capacity is utilized.

Historical Trends (Lag Features): Lag features help capture the trend by providing historical context for the current enrollment numbers.

# **STEP 4. DATA SPLITTING**

Splitting the data into training and testing sets is important for evaluation of the performance of our model how is it performing.

The training set is used to train the model, while the testing set is used to evaluate the performance of the model on unseen (new) data.

**4.1 DEFINE FEATURES (INPUT) AND TARGET (OUTPUT)**


Define the features (X) and the target variable (y). In this case, we will predict the REGISTERED column based on other features.
"""

from sklearn.model_selection import train_test_split

# Define the features as (X) and the target as (y)
# We will exclude the 'YEAR', 'COURSE', and 'REGISTERED' columns from features
features = ['SELECTED', 'CAPACITY', 'START_YEAR', 'COURSE_POPULARITY', 'CAPACITY_UTILIZATION', 'REGISTERED_LAG_1']
X = data[features]
y = data['REGISTERED']

"""**4.2 SPLITING DATA**

Split the data into training and testing sets.
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets
print(f'X_train shape: {X_train.shape}')
print(f'X_test shape: {X_test.shape}')
print(f'y_train shape: {y_train.shape}')
print(f'y_test shape: {y_test.shape}')

"""# **Step 5: MODEL SELECTION**

Selecting the right machine learning model is the important step for the success of our prediction task. Given the nature of our data is (predicting student enrollment), several regression algorithms could be suitable.

Here are some common regression algorithms to consider:

**Linear Regression:** A basic but often effective model for regression tasks.

**Decision Tree Regressor:** Captures non-linear relationships by splitting the data into different branches.

**Random Forest Regressor:** An ensemble method that builds multiple decision trees and averages their predictions.

**Gradient Boosting Regressor:** An advanced ensemble technique that builds trees sequentially to minimize errors.

**Support Vector Regressor (SVR)**: Uses support vector machines for regression tasks, effective for small to medium-sized datasets.

**IMPORT NECESSARY LIBRARIES**
"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

"""**INITIALIZING MODELS**"""

# Initialize models
linear_model = LinearRegression()
decision_tree_model = DecisionTreeRegressor(random_state=42)
random_forest_model = RandomForestRegressor(random_state=42)
gradient_boosting_model = GradientBoostingRegressor(random_state=42)

"""**TRAINING MODELS**"""

# Train the models
linear_model.fit(X_train, y_train)
decision_tree_model.fit(X_train, y_train)
random_forest_model.fit(X_train, y_train)
gradient_boosting_model.fit(X_train, y_train)

"""**EVALUATE MODELS**"""

# Make predictions on the test set
y_pred_linear = linear_model.predict(X_test)
y_pred_tree = decision_tree_model.predict(X_test)
y_pred_forest = random_forest_model.predict(X_test)
y_pred_boosting = gradient_boosting_model.predict(X_test)

# Calculate performance metrics
mse_linear = mean_squared_error(y_test, y_pred_linear)
mse_tree = mean_squared_error(y_test, y_pred_tree)
mse_forest = mean_squared_error(y_test, y_pred_forest)
mse_boosting = mean_squared_error(y_test, y_pred_boosting)

r2_linear = r2_score(y_test, y_pred_linear)
r2_tree = r2_score(y_test, y_pred_tree)
r2_forest = r2_score(y_test, y_pred_forest)
r2_boosting = r2_score(y_test, y_pred_boosting)

# Print the performance metrics
print(f"Linear Regression - MSE: {mse_linear}, R^2: {r2_linear}")
print(f"Decision Tree Regressor - MSE: {mse_tree}, R^2: {r2_tree}")
print(f"Random Forest Regressor - MSE: {mse_forest}, R^2: {r2_forest}")
print(f"Gradient Boosting Regressor - MSE: {mse_boosting}, R^2: {r2_boosting}")

"""Importing necessary libraries and models from Scikit-learn.

Initializing and train multiple models to compare their performance.

Evaluating Models: Make predictions on the test set and evaluate each model using Mean Squared Error (MSE) and R-squared (R²) metrics.

# **ANALYSIS OF THE MODELS PERFORMANCE**

From the results obtained from the above training models, it's clear that the Gradient Boosting Regressor has the best performance among the models tested (the performace of approximate 95% accurate), with the lowest Mean Squared Error (MSE) of 168 and the highest R-squared (R²) value.

Here is a summary of the results from our models:

Linear Regression:

MSE: 381.39

R-squared: 0.8773

Decision Tree Regressor:

MSE: 296.36

R-squared: 0.9047

Random Forest Regressor:

MSE: 229.32

R-squared: 0.9262


Gradient Boosting Regressor:

MSE: 168.05

R-squared: 0.9460

Since the Gradient Boosting Regressor has shown the best performance,
we should focus on tuning this model to further improve its accuracy.

Before we proceed with hyperparameter tuning, it's also beneficial to perform feature importance analysis and possibly reconsider including the COURSE column with proper encoding.

# **STEP 6: HYPERPARAMETER TUNING**

We can use GridSearchCV or RandomizedSearchCV from Scikit-learn to find the best hyperparameters for the Gradient Boosting Regressor.

Lets start by using GridSearchCV
"""

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the GridSearchCV with GradientBoostingRegressor
grid_search = GridSearchCV(
    estimator=gradient_boosting_model,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=2
    )

# Fit the GridSearchCV to the data
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Parameters: {best_params}")
print(f"Best Cross-Validation Score: {best_score}")

# Use the best estimator to make predictions on the test set
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

# Calculate the performance metrics for the best model
mse_best = mean_squared_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)

print(f"Best Gradient Boosting Regressor - MSE: {mse_best}, R^2: {r2_best}")

"""Analysis of Tuned Model Performance
The tuned Gradient Boosting Regressor has further improved performance:

Best Gradient Boosting Regressor:

MSE: 130.05

R-squared: 0.9582

These results indicate that the model performs very well, with a lower MSE and higher R-squared compared to the initial models.

# **Step 8: Cross-Validation**

Perform cross-validation to ensure that the model's performance is consistent across different subsets of the data.
"""

from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(best_model, X_train, y_train, cv=10, scoring='r2')

# Print the cross-validation scores and their mean
print(f"Cross-Validation R² Scores: {cv_scores}")
print()
print(f"Mean Cross-Validation R² Score: {cv_scores.mean()}")

"""# **Step 9: Feature Importance Analysis**

Understand which features contribute most to the model's predictions.
"""

# Get feature importances from the best model
feature_importances = best_model.feature_importances_

# Create a DataFrame for feature importances
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Display the feature importances
print(importance_df)

# Plot the feature importances
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importances')
plt.show()

"""# **ACTUAL Vs PREDICTED VALUES**

Plot the actual values versus the predicted values to see how well the model performs.
"""

import matplotlib.pyplot as plt

# Predict on the test set
y_pred = best_model.predict(X_test)

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.xlabel('Actual Registered')
plt.ylabel('Predicted Registered')
plt.title('Actual vs Predicted Registered Students')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Line of perfect fit
plt.show()

"""**2. Residual Plot**"""

residuals = y_test - y_pred

plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.6)
plt.xlabel('Predicted Registered')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.axhline(y=0, color='red', linestyle='--')
plt.show()

"""**3. Distribution of Residuals**"""

plt.figure(figsize=(10, 6))
plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals')
plt.show()

"""**4. Feature Importances**"""

import seaborn as sns

importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': best_model.feature_importances_
})

importance_df = importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importances')
plt.show()

"""**5. Time Series Plot (if applicable)**"""

# Assuming 'START_YEAR' is part of your data
plt.figure(figsize=(10, 6))
plt.plot(data['START_YEAR'], data['REGISTERED'], label='Actual')
plt.plot(data['START_YEAR'], best_model.predict(X), label='Predicted')
plt.xlabel('Year')
plt.ylabel('Registered Students')
plt.title('Registered Students Over Time')
plt.legend()
plt.show()

"""**6. Pairplot of Features**"""

sns.pairplot(data, diag_kind='kde')
plt.show()

import numpy as np
import seaborn as sns

# Calculate the errors (residuals)
errors = y_test - y_pred

# Create a larger heatmap of the errors
plt.figure(figsize=(15, 10))
sns.heatmap(np.reshape(errors, (-1, 1)), cmap='coolwarm', annot=True, cbar=True, fmt=".2f")
plt.title('Heatmap of Prediction Errors')
plt.xlabel('Error')
plt.ylabel('Test Samples')
plt.show()

"""If the heatmap reveals that some bars (samples) have higher residuals, it indicates that there are specific instances where the model's predictions are significantly off from the actual values. To gain further insights, you can:

Identify and Investigate High Residual Samples: Determine which samples have high residuals and investigate why the model performed poorly on these.
Visualize Residuals in Different Ways: Complement the heatmap with additional plots for better understanding.
Step 1: Identify High Residual Samples
Let's identify the samples with the highest residuals.

# **Step 1: Identify High Residual Samples**
"""

# Create a DataFrame to better visualize the data
error_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred,
    'Error': errors
})

# Sort the DataFrame by the absolute value of the errors
error_df['Absolute Error'] = error_df['Error'].abs()
high_residual_samples = error_df.sort_values(by='Absolute Error', ascending=False).head(10)

print(high_residual_samples)

"""# **Step 2: Visualize Residuals with Additional Plots**"""

plt.figure(figsize=(10, 6))
sns.histplot(errors, bins=30, kde=True)
plt.title('Distribution of Prediction Errors')
plt.xlabel('Error')
plt.ylabel('Frequency')
plt.show()

"""# **Actual vs. Predicted with Residuals**"""

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, label='Predicted')
plt.xlabel('Actual Registered')
plt.ylabel('Predicted Registered')
plt.title('Actual vs Predicted Registered Students')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect Fit')
plt.legend()
plt.show()

# Highlight samples with high residuals
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, label='Predicted')
plt.scatter(high_residual_samples['Actual'], high_residual_samples['Predicted'], color='red', label='High Residuals')
plt.xlabel('Actual Registered')
plt.ylabel('Predicted Registered')
plt.title('Actual vs Predicted Registered Students with High Residuals Highlighted')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect Fit')
plt.legend()
plt.show()

"""# **Step 3: Analyze Features of High Residual Samples**"""

# Extract features of high residual samples
high_residual_features = X_test.loc[high_residual_samples.index]

# Compare the high residual samples against the overall distribution
for feature in features:
    plt.figure(figsize=(10, 6))
    sns.histplot(X_test[feature], bins=30, kde=True, color='blue', label='All Samples')
    sns.histplot(high_residual_features[feature], bins=30, kde=True, color='red', label='High Residual Samples')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

#END HERE

import seaborn as sns
import matplotlib.pyplot as plt

# Plot the residuals
plt.figure(figsize=(10, 6))
sns.boxplot(y=errors)
plt.title('Boxplot of Residuals')
plt.show()

import numpy as np

# Calculate the residuals/errors
errors = y_train - best_model.predict(X_train)

# Define a threshold for considering residuals as outliers (e.g., 3 standard deviations)
threshold = 3 * np.std(errors)

# Identify outliers
outliers = np.abs(errors) > threshold

# Ensure that the index of 'outliers' aligns with 'X_train' and 'y_train'
outliers = outliers.reset_index(drop=True)
X_train = X_train.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)

# Remove outliers from the dataset
X_train_no_outliers = X_train[~outliers]
y_train_no_outliers = y_train[~outliers]

print(f"Removed {outliers.sum()} outliers")

# Retrain the model without outliers
best_model.fit(X_train_no_outliers, y_train_no_outliers)

# Predict on the test set
y_pred_no_outliers = best_model.predict(X_test)

# Evaluate the model performance
mse_no_outliers = mean_squared_error(y_test, y_pred_no_outliers)
r2_no_outliers = r2_score(y_test, y_pred_no_outliers)

print(f"Best Gradient Boosting Regressor (without outliers) - MSE: {mse_no_outliers}, R^2: {r2_no_outliers}")

# Create polynomial features
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_poly = poly.fit_transform(X_train_no_outliers)
X_test_poly = poly.transform(X_test)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

param_dist = {
    'n_estimators': randint(100, 1000),
    'learning_rate': uniform(0.01, 0.2),
    'max_depth': randint(3, 10),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 10)
}

random_search = RandomizedSearchCV(
    estimator=best_model, 
    param_distributions=param_dist, 
    n_iter=100, cv=5, n_jobs=-1, 
    verbose=2, random_state=42
    )
random_search.fit(X_train_poly, y_train_no_outliers)

best_params = random_search.best_params_
best_model = random_search.best_estimator_

# Evaluate on the test set
y_pred_tuned = best_model.predict(X_test_poly)
mse_tuned = mean_squared_error(y_test, y_pred_tuned)
r2_tuned = r2_score(y_test, y_pred_tuned)

print(f"Tuned Gradient Boosting Regressor - MSE: {mse_tuned}, R^2: {r2_tuned}")

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(best_model, X_train_poly, y_train_no_outliers, cv=5, scoring='r2')

print(f"Cross-Validation R² Scores: {cv_scores}")
print(f"Mean Cross-Validation R² Score: {cv_scores.mean()}")

import matplotlib.pyplot as plt

# Calculate the residuals/errors
errors = y_test - y_pred

# Create a scatter plot of residuals
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, errors, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.grid(True)
plt.show()

"""# **Step 1: Generate Predictions with the Improved Model**"""

# Predict on the test set using the improved model
y_pred_improved = best_model.predict(X_test_poly)

# Calculate the new residuals/errors
errors_improved = y_test - y_pred_improved

"""# **Step 2: Create and Display the DataFrame of High Residual Samples**"""

import pandas as pd

# Create a DataFrame to better visualize the data
error_df_improved = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred_improved,
    'Error': errors_improved
})

# Sort the DataFrame by the absolute value of the errors
error_df_improved['Absolute Error'] = error_df_improved['Error'].abs()
high_residual_samples_improved = error_df_improved.sort_values(by='Absolute Error', ascending=False).head(10)

print(high_residual_samples_improved)

"""# **Step 3: Create a Heatmap of the Residuals**"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Create a larger heatmap of the errors
plt.figure(figsize=(15, 10))
sns.heatmap(np.reshape(errors_improved, (-1, 1)), cmap='coolwarm', annot=True, cbar=True, fmt=".2f")
plt.title('Heatmap of Prediction Errors (Improved Model)')
plt.xlabel('Error')
plt.ylabel('Test Samples')
plt.show()

"""# **Actual vs. Predicted with Residuals**"""

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_improved, alpha=0.6, label='Predicted')
plt.xlabel('Actual Registered')
plt.ylabel('Predicted Registered')
plt.title('Actual vs Predicted Registered Students')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect Fit')
plt.legend()
plt.show()

# Highlight samples with high residuals
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_improved, alpha=0.6, label='Predicted')
plt.scatter(high_residual_samples_improved['Actual'], high_residual_samples_improved['Predicted'], color='red', label='High Residuals')
plt.xlabel('Actual Registered')
plt.ylabel('Predicted Registered')
plt.title('Actual vs Predicted Registered Students with High Residuals Highlighted (Improved)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect Fit')
plt.legend()
plt.show()

"""# **Step 3: Analyze Features of High Residual Samples**"""

# Extract features of high residual samples
high_residual_features_improved = X_test.loc[high_residual_samples_improved.index]

# Compare the high residual samples against the overall distribution
for feature in features:
    plt.figure(figsize=(10, 6))
    sns.histplot(X_test[feature], bins=30, kde=True, color='blue', label='All Samples')
    sns.histplot(high_residual_features_improved[feature], bins=30, kde=True, color='red', label='High Residual Samples')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

"""# **SAVE THE MODEL FOR THE LATER USE**"""

import joblib, pickle

# Save the model to a file
joblib.dump(best_model, 'bestmodel_for_prediction.pkl')

print("Model saved as 'bestmodel_for_prediction.pkl'")